{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# Set the global font to be DejaVu Sans, size 10 (or any other sans-serif font of your choice!)\n",
    "rc('font',**{'family':'sans-serif','sans-serif':['DejaVu Sans'],'size':20})\n",
    "\n",
    "# Set the font used for MathJax - more on this later\n",
    "rc('mathtext',**{'default':'regular'})\n",
    "\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score,ConfusionMatrixDisplay, balanced_accuracy_score, precision_score, recall_score, confusion_matrix, roc_curve, RocCurveDisplay\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut, GridSearchCV, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from numpy import sort\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE,BorderlineSMOTE,ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "DataControls = pd.read_csv('DataFromControls.csv')\n",
    "DataParkinsons = pd.read_csv('DataFromParkinsons.csv')\n",
    "DataRBD = pd.read_csv('DataFromRBD.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original {np.int64(0): np.int64(48), np.int64(1): np.int64(39)}\n"
     ]
    }
   ],
   "source": [
    "# define data groups\n",
    "\n",
    "selectedCols = ['MeanAmplitude','MeanSpeed', 'amplitudeDecay','velocityDecay']\n",
    "\n",
    "X1 = DataControls[selectedCols].values.astype(float)\n",
    "y1 = [0]*len(X1)\n",
    "\n",
    "X2 = DataRBD[selectedCols].values.astype(float)\n",
    "y2 = [1]*len(X2)\n",
    "\n",
    "\n",
    "X3 = DataParkinsons[selectedCols].values.astype(float)\n",
    "y3 = [1]*len(X3)\n",
    "\n",
    "\n",
    "XCR = np.concatenate([X1, X2], axis=0)\n",
    "yCR = np.array([0]*len(X1)+[1]*len(X2))\n",
    "\n",
    "XCP = np.concatenate([X1,X3], axis=0)\n",
    "yCP = np.array([0]*len(X1)+[1]*len(X3))\n",
    "\n",
    "\n",
    "XRP = np.concatenate([X2, X3], axis=0)\n",
    "yRP = np.array([0]*len(X2)+[1]*len(X3))\n",
    "\n",
    "X = np.concatenate([X1, X2, X3], axis=0)\n",
    "y = np.array(y1+y2+y3)\n",
    "\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print('original',dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train different binary models to classify the groups. We use the following models:\n",
    "- Logistic Regression\n",
    "- Support Vector Machines\n",
    "- Random Forest \n",
    "- Gradient Boosting\n",
    "\n",
    "\n",
    "The results of this analysis might vary from trial to trial as the SMOTE algorithm adds randomness to the data. Repeat the analysis many times to obtain an average. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented {np.int64(0): np.int64(48), np.int64(1): np.int64(48)}\n",
      "LOO CV Accuracy: 0.88\n",
      "LOO CV F1-score: 0.88\n",
      "LOO CV recall: 0.92\n",
      "LOO CV precision: 0.85\n",
      "LOO CV sensitivity: 0.92\n",
      "LOO CV specificity: 0.83\n",
      "[[40  8]\n",
      " [ 4 44]]\n"
     ]
    }
   ],
   "source": [
    "# Controls vs. PD\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE()\n",
    "under = RandomUnderSampler()\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# transform the dataset\n",
    "Xaug, yaug = pipeline.fit_resample(XCP, yCP)\n",
    "unique, counts = np.unique(yaug, return_counts=True)\n",
    "print('augmented',dict(zip(unique, counts)))   \n",
    "\n",
    "#defining the cross-validation\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "\n",
    "# Select a model, you can use any model you like. We present results with Random Forest\n",
    "\n",
    "# model, scale = LogisticRegression(), True  ##need to scale data \n",
    "# model, scale = svm.SVC(), True  ##need to scale data \n",
    "model, scale = RandomForestClassifier(), False ##no need to scale data \n",
    "# model, scale =   XGBClassifier(objective= 'binary:logistic',seed=42,n_jobs=-1,nthread=1,early_stopping_rounds=None,eval_metric='logloss',use_label_encoder=False,verbosity=0), False\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Perform LOO CV\n",
    "for train_index, test_index in cv.split(Xaug):\n",
    "    X_train, X_test = Xaug[train_index], Xaug[test_index]\n",
    "    y_train, y_test = yaug[train_index], yaug[test_index]\n",
    "\n",
    "     ## Scale the data\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    predictions.append(y_pred[0])\n",
    "    true_labels.append(y_test[0])\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = balanced_accuracy_score(true_labels, predictions)\n",
    "f1score = f1_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "sensitivity = tp /(tp+fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print(f\"LOO CV Accuracy: {accuracy:.2f}\")\n",
    "print(f\"LOO CV F1-score: {f1score:.2f}\")\n",
    "print(f\"LOO CV recall: {recall:.2f}\")\n",
    "print(f\"LOO CV precision: {precision:.2f}\")\n",
    "print(f\"LOO CV sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"LOO CV specificity: {specificity :.2f}\")\n",
    "print( confusion_matrix(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented {np.int64(0): np.int64(48), np.int64(1): np.int64(48)}\n",
      "LOO CV Accuracy: 0.80\n",
      "LOO CV F1-score: 0.81\n",
      "LOO CV recall: 0.83\n",
      "LOO CV precision: 0.78\n",
      "LOO CV sensitivity: 0.83\n",
      "LOO CV specificity: 0.77\n",
      "[[37 11]\n",
      " [ 8 40]]\n"
     ]
    }
   ],
   "source": [
    "# Controls vs. RBD\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE()\n",
    "under = RandomUnderSampler()\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# transform the dataset\n",
    "Xaug, yaug = pipeline.fit_resample(XCR, yCR)\n",
    "unique, counts = np.unique(yaug, return_counts=True)\n",
    "print('augmented',dict(zip(unique, counts)))   \n",
    "\n",
    "#defining the cross-validation\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "\n",
    "# Select a model, you can use any model you like. We present results with Random Forest\n",
    "\n",
    "# model, scale = LogisticRegression(), True  ##need to scale data \n",
    "# model, scale = svm.SVC(), True  ##need to scale data \n",
    "model, scale = RandomForestClassifier(), False ##no need to scale data \n",
    "# model, scale =   XGBClassifier(objective= 'binary:logistic',seed=42,n_jobs=-1,nthread=1,early_stopping_rounds=None,eval_metric='logloss',use_label_encoder=False,verbosity=0), False\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Perform LOO CV\n",
    "for train_index, test_index in cv.split(Xaug):\n",
    "    X_train, X_test = Xaug[train_index], Xaug[test_index]\n",
    "    y_train, y_test = yaug[train_index], yaug[test_index]\n",
    "\n",
    "    ## Scale the data or no\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    predictions.append(y_pred[0])\n",
    "    true_labels.append(y_test[0])\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = balanced_accuracy_score(true_labels, predictions)\n",
    "f1score = f1_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "sensitivity = tp /(tp+fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print(f\"LOO CV Accuracy: {accuracy:.2f}\")\n",
    "print(f\"LOO CV F1-score: {f1score:.2f}\")\n",
    "print(f\"LOO CV recall: {recall:.2f}\")\n",
    "print(f\"LOO CV precision: {precision:.2f}\")\n",
    "print(f\"LOO CV sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"LOO CV specificity: {specificity :.2f}\")\n",
    "print( confusion_matrix(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "augmented {np.int64(0): np.int64(21), np.int64(1): np.int64(21)}\n",
      "LOO CV Accuracy: 0.74\n",
      "LOO CV F1-score: 0.74\n",
      "LOO CV recall: 0.76\n",
      "LOO CV precision: 0.73\n",
      "LOO CV sensitivity: 0.76\n",
      "LOO CV specificity: 0.71\n",
      "[[15  6]\n",
      " [ 5 16]]\n"
     ]
    }
   ],
   "source": [
    "# RBD vs. PD\n",
    "\n",
    "# define pipeline\n",
    "over = SMOTE()\n",
    "under = RandomUnderSampler()\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# transform the dataset\n",
    "Xaug, yaug = pipeline.fit_resample(XRP, yRP)\n",
    "unique, counts = np.unique(yaug, return_counts=True)\n",
    "print('augmented',dict(zip(unique, counts)))   \n",
    "\n",
    "#defining the cross-validation\n",
    "cv = LeaveOneOut()\n",
    "\n",
    "\n",
    "# Select a model, you can use any model you like. We present results with Random Forest\n",
    "\n",
    "# model, scale = LogisticRegression(), True  ##need to scale data \n",
    "# model, scale = svm.SVC(), True  ##need to scale data \n",
    "model, scale = RandomForestClassifier(), False ##no need to scale data \n",
    "# model, scale =   XGBClassifier(objective= 'binary:logistic',seed=42,n_jobs=-1,nthread=1,early_stopping_rounds=None,eval_metric='logloss',use_label_encoder=False,verbosity=0), False\n",
    "\n",
    "# Initialize list to store predictions\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# Perform LOO CV\n",
    "for train_index, test_index in cv.split(Xaug):\n",
    "    X_train, X_test = Xaug[train_index], Xaug[test_index]\n",
    "    y_train, y_test = yaug[train_index], yaug[test_index]\n",
    "\n",
    "    ## Scale the data or no\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "        # Fit the model\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    \n",
    "    predictions.append(y_pred[0])\n",
    "    true_labels.append(y_test[0])\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = balanced_accuracy_score(true_labels, predictions)\n",
    "f1score = f1_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "sensitivity = tp /(tp+fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print(f\"LOO CV Accuracy: {accuracy:.2f}\")\n",
    "print(f\"LOO CV F1-score: {f1score:.2f}\")\n",
    "print(f\"LOO CV recall: {recall:.2f}\")\n",
    "print(f\"LOO CV precision: {precision:.2f}\")\n",
    "print(f\"LOO CV sensitivity: {sensitivity:.2f}\")\n",
    "print(f\"LOO CV specificity: {specificity :.2f}\")\n",
    "print( confusion_matrix(true_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
